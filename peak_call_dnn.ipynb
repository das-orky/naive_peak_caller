{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import logomaker\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos=[]\n",
    "path='/home/ubuntu/tmp/data/pos/'\n",
    "for filename in glob.glob(os.path.join(path,'*.txt')):\n",
    "    with open(os.path.join(os.getcwd(),filename),'r') as file:\n",
    "        while line := file.readline():\n",
    "            data_pos.append(line)   \n",
    "\n",
    "data_neg=[]\n",
    "path='/home/ubuntu/tmp/data/neg/'\n",
    "for filename in glob.glob(os.path.join(path,'*.txt')):\n",
    "    with open(os.path.join(os.getcwd(),filename),'r') as file:\n",
    "        while line := file.readline():\n",
    "            data_neg.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually the number of negative data points will be more than the positive data point, since positive data is created\n",
    "# by applying filter to the raw data. Whereas negative data points are also created by applying filter to the\n",
    "# raw data, but the conditions are less stringent. This logic may change in the future.\n",
    "\n",
    "total_rows=len(data_neg)+len(data_pos)\n",
    "merge=[[[] for x in range(2)] for y in range(0,total_rows)]\n",
    "index=0\n",
    "diff=abs(len(data_neg)-len(data_pos))\n",
    "for i in range(0,len(data_pos)):\n",
    "    merge[index][0]=data_pos[i].strip().upper()\n",
    "    merge[index][1]=1.0\n",
    "    index=index+1\n",
    "    merge[index][0]=data_neg[i].strip().upper()\n",
    "    merge[index][1]=0.0\n",
    "    index=index+1\n",
    "    if diff >= 1:\n",
    "        merge[index][0]=data_neg[i].strip().upper()\n",
    "        merge[index][1]=0.0\n",
    "        index=index+1\n",
    "        diff = diff -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AGGAAAACGTGAATCTATGTGGACTGTTCCAAACAATCCCAATTCCCCAGCTAATGAGCTCAAAGCTTTGGAAACAGGGAAAATGTCAAAGGATCCCGATTCGCCAGCTAATGAGCTGAAAGGCAATGAACCAGGAGAAGTGTCAAAAGA'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge[5565][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13190"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_neg)+len(data_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the nucleotide are converted into a upper case letter. There are instance where the sequence may contain both upper and lower letter.\n",
    "# Lower and Upper case has different interpretation, lower may be \"soft mask\" not completely align to the region. Lower case also means that\n",
    "# it may be a repetative statement. In case of gene sequence it may mean introns and Exons (Upper). \n",
    "# If N/n is there then it will be assigned a value of [0.,0.,0.,0.]\n",
    "\n",
    "def dna_to_one_hot_encoding(genome):\n",
    "    one_hot=list()\n",
    "    nucli_map={\"A\":[1., 0., 0., 0.], \"C\": [0., 1., 0., 0.], \"G\": [0., 0., 1., 0.], \"T\":[0., 0., 0., 1.]}\n",
    "    #print(genome)\n",
    "    for nucleotide in genome:\n",
    "        one_hot.append(nucli_map[nucleotide]  if nucleotide in nucli_map.keys() else [0., 0., 0., 0.])\n",
    "\n",
    "    return np.array(one_hot).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=dna_to_one_hot_encoding(merge[13189][0])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all the indexes\n",
    "\n",
    "index_shuffle=list(range(total_rows))\n",
    "random.shuffle(index_shuffle)\n",
    "\n",
    "train_percentage=0.7\n",
    "train_split=int(len(index_shuffle)*train_percentage)\n",
    "remaining=total_rows-train_split\n",
    "validation_split=int(remaining*0.7)\n",
    "\n",
    "train_idx=index_shuffle[:train_split]\n",
    "validation_idx=index_shuffle[train_split:train_split+validation_split]\n",
    "test_idx=index_shuffle[train_split+validation_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the sequences and its respective label from the indexs what is already calculated for the three different splits \n",
    "\n",
    "train_data_raw=[]\n",
    "for i in train_idx:\n",
    "    train_data_raw.append(merge[i])\n",
    "\n",
    "validation_data_raw=[]\n",
    "for i in validation_idx:\n",
    "    validation_data_raw.append(merge[i])\n",
    "\n",
    "test_data_raw=[]\n",
    "for i in test_idx:\n",
    "    test_data_raw.append(merge[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom Dataset class. The input to the function genome is a 2D data set, where the 0th dimension of an element is the sequence and\n",
    "# 1st dimension is the label of the sequence. That is why x[0] is used, which contain the sequence part and x[1] contain the label. \n",
    "\n",
    "class genome_to_one_hot(Dataset):\n",
    "    def __init__(self,genome):\n",
    "        self.dna=genome\n",
    "\n",
    "        self.genome_one_hot=torch.stack([torch.tensor(dna_to_one_hot_encoding(x[0])) for x in self.dna])\n",
    "        self.labels=torch.tensor(list(x[1] for x in self.dna))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dna)\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        one_hot_dna=self.genome_one_hot[id]\n",
    "        labels=self.labels[id]\n",
    "\n",
    "        return one_hot_dna,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "\n",
    "test_data=genome_to_one_hot(test_data_raw)\n",
    "train_data=genome_to_one_hot(train_data_raw)\n",
    "validation_data=genome_to_one_hot(validation_data_raw)\n",
    "\n",
    "train_dataloader=DataLoader(train_data,batch_size=512,shuffle=True)\n",
    "validation_dataloader=DataLoader(validation_data,batch_size=512,shuffle=True)\n",
    "test_dataloader=DataLoader(test_data,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_peak_cnn(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=4,out_channels=32,kernel_size=11,stride=1,padding=0)\n",
    "        self.pool1=nn.MaxPool1d(kernel_size=3,stride=2)\n",
    "        self.conv2=nn.Conv1d(in_channels=32,out_channels=64,kernel_size=5,stride=1,padding=0)\n",
    "        self.pool2=nn.MaxPool1d(kernel_size=3,stride=2)\n",
    "        self.conv3=nn.Conv1d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=0)\n",
    "        self.pool3=nn.MaxPool1d(kernel_size=3,stride=2)\n",
    "        self.conv4=nn.Conv1d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=0)\n",
    "        self.pool4=nn.MaxPool1d(kernel_size=3,stride=2)\n",
    "\n",
    "        self.fc1=nn.Linear(in_features=(128*5),out_features=2048)\n",
    "        self.fc2=nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.fc3=nn.Linear(in_features=2048,out_features=num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool1(F.relu(self.conv1(x)))\n",
    "        x=self.pool2(F.relu(self.conv2(x)))\n",
    "        x=self.pool3(F.relu(self.conv3(x)))\n",
    "        x=self.pool4(F.relu(self.conv4(x)))\n",
    "\n",
    "        x=torch.flatten(x,1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.dropout(x,0.6)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.dropout(x,0.6)\n",
    "        x=F.relu(self.fc3(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=deep_peak_cnn().to(device)\n",
    "optimizer=optim.Adam(params=model.parameters(),lr=0.0001)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to float\n",
    "\n",
    "def train(model,train_loader,optimizer,epochs,device):\n",
    "    model.train()\n",
    "    for batch_ids, (data,labels) in enumerate(train_loader):\n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        data,labels=data.to(device, dtype=torch.float32),labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_output=model(data)\n",
    "        loss=loss_fn(model_output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_ids+1)%6 == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epochs,batch_ids*len(data),len(train_loader.dataset),\n",
    "                100.*batch_ids/len(train_loader),loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score is also calculated. \n",
    "\n",
    "def validate(model, validation_loader,device):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data,labels in validation_loader:\n",
    "            data,labels=data.to(device,dtype=torch.float32), labels.to(device,dtype=torch.float32)\n",
    "            y_hat=model(data)\n",
    "            _,y_pred=torch.max(y_hat,1)\n",
    "            correct+=(y_pred==labels).sum().item()\n",
    "            #print(\"f1_score:\", f1_score(labels.cpu().data,y_pred.cpu()))\n",
    "        print(\"\\n Test Set: Average loss: xx , Accuracy:{}/{} ({:.0f}%, f1_score:{})\".format(\n",
    "            correct,len(validation_data),100.*correct/len(validation_data),f1_score(labels.cpu().data,y_pred.cpu())))\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2560/9233 (26%)]\tLoss: 0.683108\n",
      "Train Epoch: 1 [5632/9233 (58%)]\tLoss: 0.681863\n",
      "Train Epoch: 1 [8704/9233 (89%)]\tLoss: 0.673304\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 2 [2560/9233 (26%)]\tLoss: 0.674464\n",
      "Train Epoch: 2 [5632/9233 (58%)]\tLoss: 0.687068\n",
      "Train Epoch: 2 [8704/9233 (89%)]\tLoss: 0.673895\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 3 [2560/9233 (26%)]\tLoss: 0.664211\n",
      "Train Epoch: 3 [5632/9233 (58%)]\tLoss: 0.667384\n",
      "Train Epoch: 3 [8704/9233 (89%)]\tLoss: 0.662929\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 4 [2560/9233 (26%)]\tLoss: 0.676531\n",
      "Train Epoch: 4 [5632/9233 (58%)]\tLoss: 0.680842\n",
      "Train Epoch: 4 [8704/9233 (89%)]\tLoss: 0.665258\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 5 [2560/9233 (26%)]\tLoss: 0.669793\n",
      "Train Epoch: 5 [5632/9233 (58%)]\tLoss: 0.658925\n",
      "Train Epoch: 5 [8704/9233 (89%)]\tLoss: 0.662172\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 6 [2560/9233 (26%)]\tLoss: 0.674142\n",
      "Train Epoch: 6 [5632/9233 (58%)]\tLoss: 0.640136\n",
      "Train Epoch: 6 [8704/9233 (89%)]\tLoss: 0.647291\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 7 [2560/9233 (26%)]\tLoss: 0.663388\n",
      "Train Epoch: 7 [5632/9233 (58%)]\tLoss: 0.652276\n",
      "Train Epoch: 7 [8704/9233 (89%)]\tLoss: 0.649166\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1583/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 8 [2560/9233 (26%)]\tLoss: 0.662891\n",
      "Train Epoch: 8 [5632/9233 (58%)]\tLoss: 0.655654\n",
      "Train Epoch: 8 [8704/9233 (89%)]\tLoss: 0.651040\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 9 [2560/9233 (26%)]\tLoss: 0.662985\n",
      "Train Epoch: 9 [5632/9233 (58%)]\tLoss: 0.627142\n",
      "Train Epoch: 9 [8704/9233 (89%)]\tLoss: 0.618497\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1581/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 10 [2560/9233 (26%)]\tLoss: 0.651034\n",
      "Train Epoch: 10 [5632/9233 (58%)]\tLoss: 0.632449\n",
      "Train Epoch: 10 [8704/9233 (89%)]\tLoss: 0.650547\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1582/2769 (57%, f1_score:0.0)\n",
      "==================================================\n",
      "Train Epoch: 11 [2560/9233 (26%)]\tLoss: 0.639039\n",
      "Train Epoch: 11 [5632/9233 (58%)]\tLoss: 0.618560\n",
      "Train Epoch: 11 [8704/9233 (89%)]\tLoss: 0.649935\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1588/2769 (57%, f1_score:0.02247191011235955)\n",
      "==================================================\n",
      "Train Epoch: 12 [2560/9233 (26%)]\tLoss: 0.648388\n",
      "Train Epoch: 12 [5632/9233 (58%)]\tLoss: 0.650017\n",
      "Train Epoch: 12 [8704/9233 (89%)]\tLoss: 0.644478\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1618/2769 (58%, f1_score:0.0925925925925926)\n",
      "==================================================\n",
      "Train Epoch: 13 [2560/9233 (26%)]\tLoss: 0.631997\n",
      "Train Epoch: 13 [5632/9233 (58%)]\tLoss: 0.633009\n",
      "Train Epoch: 13 [8704/9233 (89%)]\tLoss: 0.651208\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1684/2769 (61%, f1_score:0.4795321637426901)\n",
      "==================================================\n",
      "Train Epoch: 14 [2560/9233 (26%)]\tLoss: 0.660740\n",
      "Train Epoch: 14 [5632/9233 (58%)]\tLoss: 0.650216\n",
      "Train Epoch: 14 [8704/9233 (89%)]\tLoss: 0.636015\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1707/2769 (62%, f1_score:0.4358974358974359)\n",
      "==================================================\n",
      "Train Epoch: 15 [2560/9233 (26%)]\tLoss: 0.665359\n",
      "Train Epoch: 15 [5632/9233 (58%)]\tLoss: 0.613807\n",
      "Train Epoch: 15 [8704/9233 (89%)]\tLoss: 0.644287\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1655/2769 (60%, f1_score:0.5157894736842106)\n",
      "==================================================\n",
      "Train Epoch: 16 [2560/9233 (26%)]\tLoss: 0.640006\n",
      "Train Epoch: 16 [5632/9233 (58%)]\tLoss: 0.631524\n",
      "Train Epoch: 16 [8704/9233 (89%)]\tLoss: 0.634440\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1665/2769 (60%, f1_score:0.59375)\n",
      "==================================================\n",
      "Train Epoch: 17 [2560/9233 (26%)]\tLoss: 0.615508\n",
      "Train Epoch: 17 [5632/9233 (58%)]\tLoss: 0.630008\n",
      "Train Epoch: 17 [8704/9233 (89%)]\tLoss: 0.634293\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1746/2769 (63%, f1_score:0.5238095238095238)\n",
      "==================================================\n",
      "Train Epoch: 18 [2560/9233 (26%)]\tLoss: 0.617399\n",
      "Train Epoch: 18 [5632/9233 (58%)]\tLoss: 0.634381\n",
      "Train Epoch: 18 [8704/9233 (89%)]\tLoss: 0.607404\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1772/2769 (64%, f1_score:0.5319148936170214)\n",
      "==================================================\n",
      "Train Epoch: 19 [2560/9233 (26%)]\tLoss: 0.630179\n",
      "Train Epoch: 19 [5632/9233 (58%)]\tLoss: 0.636824\n",
      "Train Epoch: 19 [8704/9233 (89%)]\tLoss: 0.585491\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1699/2769 (61%, f1_score:0.14285714285714285)\n",
      "==================================================\n",
      "Train Epoch: 20 [2560/9233 (26%)]\tLoss: 0.616495\n",
      "Train Epoch: 20 [5632/9233 (58%)]\tLoss: 0.603627\n",
      "Train Epoch: 20 [8704/9233 (89%)]\tLoss: 0.601648\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1779/2769 (64%, f1_score:0.625)\n",
      "==================================================\n",
      "Train Epoch: 21 [2560/9233 (26%)]\tLoss: 0.599819\n",
      "Train Epoch: 21 [5632/9233 (58%)]\tLoss: 0.553784\n",
      "Train Epoch: 21 [8704/9233 (89%)]\tLoss: 0.595824\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1815/2769 (66%, f1_score:0.5413533834586466)\n",
      "==================================================\n",
      "Train Epoch: 22 [2560/9233 (26%)]\tLoss: 0.587480\n",
      "Train Epoch: 22 [5632/9233 (58%)]\tLoss: 0.590917\n",
      "Train Epoch: 22 [8704/9233 (89%)]\tLoss: 0.566877\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1820/2769 (66%, f1_score:0.48611111111111116)\n",
      "==================================================\n",
      "Train Epoch: 23 [2560/9233 (26%)]\tLoss: 0.605379\n",
      "Train Epoch: 23 [5632/9233 (58%)]\tLoss: 0.546686\n",
      "Train Epoch: 23 [8704/9233 (89%)]\tLoss: 0.594399\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1747/2769 (63%, f1_score:0.411214953271028)\n",
      "==================================================\n",
      "Train Epoch: 24 [2560/9233 (26%)]\tLoss: 0.619826\n",
      "Train Epoch: 24 [5632/9233 (58%)]\tLoss: 0.572616\n",
      "Train Epoch: 24 [8704/9233 (89%)]\tLoss: 0.563725\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1760/2769 (64%, f1_score:0.544502617801047)\n",
      "==================================================\n",
      "Train Epoch: 25 [2560/9233 (26%)]\tLoss: 0.566521\n",
      "Train Epoch: 25 [5632/9233 (58%)]\tLoss: 0.591900\n",
      "Train Epoch: 25 [8704/9233 (89%)]\tLoss: 0.565020\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1845/2769 (67%, f1_score:0.6282051282051282)\n",
      "==================================================\n",
      "Train Epoch: 26 [2560/9233 (26%)]\tLoss: 0.552233\n",
      "Train Epoch: 26 [5632/9233 (58%)]\tLoss: 0.541407\n",
      "Train Epoch: 26 [8704/9233 (89%)]\tLoss: 0.526951\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1837/2769 (66%, f1_score:0.608187134502924)\n",
      "==================================================\n",
      "Train Epoch: 27 [2560/9233 (26%)]\tLoss: 0.569651\n",
      "Train Epoch: 27 [5632/9233 (58%)]\tLoss: 0.569724\n",
      "Train Epoch: 27 [8704/9233 (89%)]\tLoss: 0.528750\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1866/2769 (67%, f1_score:0.6559139784946236)\n",
      "==================================================\n",
      "Train Epoch: 28 [2560/9233 (26%)]\tLoss: 0.533398\n",
      "Train Epoch: 28 [5632/9233 (58%)]\tLoss: 0.542074\n",
      "Train Epoch: 28 [8704/9233 (89%)]\tLoss: 0.582030\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1844/2769 (67%, f1_score:0.558659217877095)\n",
      "==================================================\n",
      "Train Epoch: 29 [2560/9233 (26%)]\tLoss: 0.530698\n",
      "Train Epoch: 29 [5632/9233 (58%)]\tLoss: 0.532212\n",
      "Train Epoch: 29 [8704/9233 (89%)]\tLoss: 0.580515\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1783/2769 (64%, f1_score:0.2962962962962963)\n",
      "==================================================\n",
      "Train Epoch: 30 [2560/9233 (26%)]\tLoss: 0.555371\n",
      "Train Epoch: 30 [5632/9233 (58%)]\tLoss: 0.530939\n",
      "Train Epoch: 30 [8704/9233 (89%)]\tLoss: 0.499514\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1887/2769 (68%, f1_score:0.6666666666666667)\n",
      "==================================================\n",
      "Train Epoch: 31 [2560/9233 (26%)]\tLoss: 0.543071\n",
      "Train Epoch: 31 [5632/9233 (58%)]\tLoss: 0.523371\n",
      "Train Epoch: 31 [8704/9233 (89%)]\tLoss: 0.516584\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1842/2769 (67%, f1_score:0.42028985507246375)\n",
      "==================================================\n",
      "Train Epoch: 32 [2560/9233 (26%)]\tLoss: 0.505281\n",
      "Train Epoch: 32 [5632/9233 (58%)]\tLoss: 0.530882\n",
      "Train Epoch: 32 [8704/9233 (89%)]\tLoss: 0.506498\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1779/2769 (64%, f1_score:0.35514018691588783)\n",
      "==================================================\n",
      "Train Epoch: 33 [2560/9233 (26%)]\tLoss: 0.535717\n",
      "Train Epoch: 33 [5632/9233 (58%)]\tLoss: 0.518512\n",
      "Train Epoch: 33 [8704/9233 (89%)]\tLoss: 0.560256\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1854/2769 (67%, f1_score:0.5863874345549738)\n",
      "==================================================\n",
      "Train Epoch: 34 [2560/9233 (26%)]\tLoss: 0.541100\n",
      "Train Epoch: 34 [5632/9233 (58%)]\tLoss: 0.528233\n",
      "Train Epoch: 34 [8704/9233 (89%)]\tLoss: 0.470717\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1869/2769 (67%, f1_score:0.5792349726775956)\n",
      "==================================================\n",
      "Train Epoch: 35 [2560/9233 (26%)]\tLoss: 0.487489\n",
      "Train Epoch: 35 [5632/9233 (58%)]\tLoss: 0.482097\n",
      "Train Epoch: 35 [8704/9233 (89%)]\tLoss: 0.485100\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1747/2769 (63%, f1_score:0.2247191011235955)\n",
      "==================================================\n",
      "Train Epoch: 36 [2560/9233 (26%)]\tLoss: 0.512894\n",
      "Train Epoch: 36 [5632/9233 (58%)]\tLoss: 0.509368\n",
      "Train Epoch: 36 [8704/9233 (89%)]\tLoss: 0.481137\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1892/2769 (68%, f1_score:0.42962962962962964)\n",
      "==================================================\n",
      "Train Epoch: 37 [2560/9233 (26%)]\tLoss: 0.495577\n",
      "Train Epoch: 37 [5632/9233 (58%)]\tLoss: 0.506965\n",
      "Train Epoch: 37 [8704/9233 (89%)]\tLoss: 0.521550\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1726/2769 (62%, f1_score:0.6009852216748769)\n",
      "==================================================\n",
      "Train Epoch: 38 [2560/9233 (26%)]\tLoss: 0.462001\n",
      "Train Epoch: 38 [5632/9233 (58%)]\tLoss: 0.473897\n",
      "Train Epoch: 38 [8704/9233 (89%)]\tLoss: 0.472332\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1889/2769 (68%, f1_score:0.5987261146496816)\n",
      "==================================================\n",
      "Train Epoch: 39 [2560/9233 (26%)]\tLoss: 0.509506\n",
      "Train Epoch: 39 [5632/9233 (58%)]\tLoss: 0.475299\n",
      "Train Epoch: 39 [8704/9233 (89%)]\tLoss: 0.477671\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1759/2769 (64%, f1_score:0.6302521008403361)\n",
      "==================================================\n",
      "Train Epoch: 40 [2560/9233 (26%)]\tLoss: 0.458023\n",
      "Train Epoch: 40 [5632/9233 (58%)]\tLoss: 0.457274\n",
      "Train Epoch: 40 [8704/9233 (89%)]\tLoss: 0.462607\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1923/2769 (69%, f1_score:0.6033519553072626)\n",
      "==================================================\n",
      "Train Epoch: 41 [2560/9233 (26%)]\tLoss: 0.452392\n",
      "Train Epoch: 41 [5632/9233 (58%)]\tLoss: 0.436271\n",
      "Train Epoch: 41 [8704/9233 (89%)]\tLoss: 0.452516\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1824/2769 (66%, f1_score:0.43795620437956206)\n",
      "==================================================\n",
      "Train Epoch: 42 [2560/9233 (26%)]\tLoss: 0.465025\n",
      "Train Epoch: 42 [5632/9233 (58%)]\tLoss: 0.463893\n",
      "Train Epoch: 42 [8704/9233 (89%)]\tLoss: 0.484681\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1890/2769 (68%, f1_score:0.6)\n",
      "==================================================\n",
      "Train Epoch: 43 [2560/9233 (26%)]\tLoss: 0.404648\n",
      "Train Epoch: 43 [5632/9233 (58%)]\tLoss: 0.442979\n",
      "Train Epoch: 43 [8704/9233 (89%)]\tLoss: 0.443819\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1790/2769 (65%, f1_score:0.6237623762376238)\n",
      "==================================================\n",
      "Train Epoch: 44 [2560/9233 (26%)]\tLoss: 0.429662\n",
      "Train Epoch: 44 [5632/9233 (58%)]\tLoss: 0.434462\n",
      "Train Epoch: 44 [8704/9233 (89%)]\tLoss: 0.495894\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1844/2769 (67%, f1_score:0.38805970149253727)\n",
      "==================================================\n",
      "Train Epoch: 45 [2560/9233 (26%)]\tLoss: 0.424167\n",
      "Train Epoch: 45 [5632/9233 (58%)]\tLoss: 0.471534\n",
      "Train Epoch: 45 [8704/9233 (89%)]\tLoss: 0.391699\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1920/2769 (69%, f1_score:0.6086956521739131)\n",
      "==================================================\n",
      "Train Epoch: 46 [2560/9233 (26%)]\tLoss: 0.402947\n",
      "Train Epoch: 46 [5632/9233 (58%)]\tLoss: 0.445637\n",
      "Train Epoch: 46 [8704/9233 (89%)]\tLoss: 0.428666\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1858/2769 (67%, f1_score:0.6170212765957447)\n",
      "==================================================\n",
      "Train Epoch: 47 [2560/9233 (26%)]\tLoss: 0.362179\n",
      "Train Epoch: 47 [5632/9233 (58%)]\tLoss: 0.451695\n",
      "Train Epoch: 47 [8704/9233 (89%)]\tLoss: 0.418579\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1766/2769 (64%, f1_score:0.6752136752136753)\n",
      "==================================================\n",
      "Train Epoch: 48 [2560/9233 (26%)]\tLoss: 0.418393\n",
      "Train Epoch: 48 [5632/9233 (58%)]\tLoss: 0.365915\n",
      "Train Epoch: 48 [8704/9233 (89%)]\tLoss: 0.409112\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1883/2769 (68%, f1_score:0.5921052631578948)\n",
      "==================================================\n",
      "Train Epoch: 49 [2560/9233 (26%)]\tLoss: 0.365140\n",
      "Train Epoch: 49 [5632/9233 (58%)]\tLoss: 0.380586\n",
      "Train Epoch: 49 [8704/9233 (89%)]\tLoss: 0.382261\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1903/2769 (69%, f1_score:0.5974025974025974)\n",
      "==================================================\n",
      "Train Epoch: 50 [2560/9233 (26%)]\tLoss: 0.387976\n",
      "Train Epoch: 50 [5632/9233 (58%)]\tLoss: 0.466618\n",
      "Train Epoch: 50 [8704/9233 (89%)]\tLoss: 0.408078\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1919/2769 (69%, f1_score:0.5771812080536913)\n",
      "==================================================\n",
      "Train Epoch: 51 [2560/9233 (26%)]\tLoss: 0.377374\n",
      "Train Epoch: 51 [5632/9233 (58%)]\tLoss: 0.382524\n",
      "Train Epoch: 51 [8704/9233 (89%)]\tLoss: 0.449853\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1874/2769 (68%, f1_score:0.4848484848484848)\n",
      "==================================================\n",
      "Train Epoch: 52 [2560/9233 (26%)]\tLoss: 0.435675\n",
      "Train Epoch: 52 [5632/9233 (58%)]\tLoss: 0.361667\n",
      "Train Epoch: 52 [8704/9233 (89%)]\tLoss: 0.370122\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1926/2769 (70%, f1_score:0.7037037037037037)\n",
      "==================================================\n",
      "Train Epoch: 53 [2560/9233 (26%)]\tLoss: 0.368146\n",
      "Train Epoch: 53 [5632/9233 (58%)]\tLoss: 0.377211\n",
      "Train Epoch: 53 [8704/9233 (89%)]\tLoss: 0.319865\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1626/2769 (59%, f1_score:0.5663716814159292)\n",
      "==================================================\n",
      "Train Epoch: 54 [2560/9233 (26%)]\tLoss: 0.476844\n",
      "Train Epoch: 54 [5632/9233 (58%)]\tLoss: 0.435779\n",
      "Train Epoch: 54 [8704/9233 (89%)]\tLoss: 0.379940\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1936/2769 (70%, f1_score:0.5798816568047337)\n",
      "==================================================\n",
      "Train Epoch: 55 [2560/9233 (26%)]\tLoss: 0.308184\n",
      "Train Epoch: 55 [5632/9233 (58%)]\tLoss: 0.364166\n",
      "Train Epoch: 55 [8704/9233 (89%)]\tLoss: 0.365163\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1912/2769 (69%, f1_score:0.5753424657534246)\n",
      "==================================================\n",
      "Train Epoch: 56 [2560/9233 (26%)]\tLoss: 0.342340\n",
      "Train Epoch: 56 [5632/9233 (58%)]\tLoss: 0.332644\n",
      "Train Epoch: 56 [8704/9233 (89%)]\tLoss: 0.343301\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1878/2769 (68%, f1_score:0.46153846153846156)\n",
      "==================================================\n",
      "Train Epoch: 57 [2560/9233 (26%)]\tLoss: 0.390740\n",
      "Train Epoch: 57 [5632/9233 (58%)]\tLoss: 0.341437\n",
      "Train Epoch: 57 [8704/9233 (89%)]\tLoss: 0.316740\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1912/2769 (69%, f1_score:0.6779661016949153)\n",
      "==================================================\n",
      "Train Epoch: 58 [2560/9233 (26%)]\tLoss: 0.294772\n",
      "Train Epoch: 58 [5632/9233 (58%)]\tLoss: 0.287091\n",
      "Train Epoch: 58 [8704/9233 (89%)]\tLoss: 0.329204\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1541/2769 (56%, f1_score:0.6468401486988847)\n",
      "==================================================\n",
      "Train Epoch: 59 [2560/9233 (26%)]\tLoss: 0.399170\n",
      "Train Epoch: 59 [5632/9233 (58%)]\tLoss: 0.411566\n",
      "Train Epoch: 59 [8704/9233 (89%)]\tLoss: 0.327912\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1898/2769 (69%, f1_score:0.575)\n",
      "==================================================\n",
      "Train Epoch: 60 [2560/9233 (26%)]\tLoss: 0.336418\n",
      "Train Epoch: 60 [5632/9233 (58%)]\tLoss: 0.358167\n",
      "Train Epoch: 60 [8704/9233 (89%)]\tLoss: 0.334833\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1920/2769 (69%, f1_score:0.6395348837209303)\n",
      "==================================================\n",
      "Train Epoch: 61 [2560/9233 (26%)]\tLoss: 0.287715\n",
      "Train Epoch: 61 [5632/9233 (58%)]\tLoss: 0.323055\n",
      "Train Epoch: 61 [8704/9233 (89%)]\tLoss: 0.307787\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1906/2769 (69%, f1_score:0.5359477124183005)\n",
      "==================================================\n",
      "Train Epoch: 62 [2560/9233 (26%)]\tLoss: 0.300135\n",
      "Train Epoch: 62 [5632/9233 (58%)]\tLoss: 0.282091\n",
      "Train Epoch: 62 [8704/9233 (89%)]\tLoss: 0.275685\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1748/2769 (63%, f1_score:0.6146788990825688)\n",
      "==================================================\n",
      "Train Epoch: 63 [2560/9233 (26%)]\tLoss: 0.307296\n",
      "Train Epoch: 63 [5632/9233 (58%)]\tLoss: 0.358571\n",
      "Train Epoch: 63 [8704/9233 (89%)]\tLoss: 0.308068\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1896/2769 (68%, f1_score:0.6000000000000001)\n",
      "==================================================\n",
      "Train Epoch: 64 [2560/9233 (26%)]\tLoss: 0.306558\n",
      "Train Epoch: 64 [5632/9233 (58%)]\tLoss: 0.323338\n",
      "Train Epoch: 64 [8704/9233 (89%)]\tLoss: 0.252146\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1945/2769 (70%, f1_score:0.5925925925925926)\n",
      "==================================================\n",
      "Train Epoch: 65 [2560/9233 (26%)]\tLoss: 0.281784\n",
      "Train Epoch: 65 [5632/9233 (58%)]\tLoss: 0.245612\n",
      "Train Epoch: 65 [8704/9233 (89%)]\tLoss: 0.245968\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1931/2769 (70%, f1_score:0.5673758865248226)\n",
      "==================================================\n",
      "Train Epoch: 66 [2560/9233 (26%)]\tLoss: 0.224277\n",
      "Train Epoch: 66 [5632/9233 (58%)]\tLoss: 0.242485\n",
      "Train Epoch: 66 [8704/9233 (89%)]\tLoss: 0.251447\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1707/2769 (62%, f1_score:0.6751054852320676)\n",
      "==================================================\n",
      "Train Epoch: 67 [2560/9233 (26%)]\tLoss: 0.295354\n",
      "Train Epoch: 67 [5632/9233 (58%)]\tLoss: 0.286890\n",
      "Train Epoch: 67 [8704/9233 (89%)]\tLoss: 0.240938\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1901/2769 (69%, f1_score:0.5972222222222222)\n",
      "==================================================\n",
      "Train Epoch: 68 [2560/9233 (26%)]\tLoss: 0.213217\n",
      "Train Epoch: 68 [5632/9233 (58%)]\tLoss: 0.271500\n",
      "Train Epoch: 68 [8704/9233 (89%)]\tLoss: 0.263529\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1835/2769 (66%, f1_score:0.660377358490566)\n",
      "==================================================\n",
      "Train Epoch: 69 [2560/9233 (26%)]\tLoss: 0.274281\n",
      "Train Epoch: 69 [5632/9233 (58%)]\tLoss: 0.283385\n",
      "Train Epoch: 69 [8704/9233 (89%)]\tLoss: 0.258148\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1935/2769 (70%, f1_score:0.6179775280898876)\n",
      "==================================================\n",
      "Train Epoch: 70 [2560/9233 (26%)]\tLoss: 0.249688\n",
      "Train Epoch: 70 [5632/9233 (58%)]\tLoss: 0.253207\n",
      "Train Epoch: 70 [8704/9233 (89%)]\tLoss: 0.186401\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1962/2769 (71%, f1_score:0.6163522012578616)\n",
      "==================================================\n",
      "Train Epoch: 71 [2560/9233 (26%)]\tLoss: 0.249792\n",
      "Train Epoch: 71 [5632/9233 (58%)]\tLoss: 0.238632\n",
      "Train Epoch: 71 [8704/9233 (89%)]\tLoss: 0.240722\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1947/2769 (70%, f1_score:0.5526315789473685)\n",
      "==================================================\n",
      "Train Epoch: 72 [2560/9233 (26%)]\tLoss: 0.198653\n",
      "Train Epoch: 72 [5632/9233 (58%)]\tLoss: 0.228049\n",
      "Train Epoch: 72 [8704/9233 (89%)]\tLoss: 0.212806\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1955/2769 (71%, f1_score:0.6459627329192545)\n",
      "==================================================\n",
      "Train Epoch: 73 [2560/9233 (26%)]\tLoss: 0.234797\n",
      "Train Epoch: 73 [5632/9233 (58%)]\tLoss: 0.198717\n",
      "Train Epoch: 73 [8704/9233 (89%)]\tLoss: 0.202666\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1949/2769 (70%, f1_score:0.5906040268456376)\n",
      "==================================================\n",
      "Train Epoch: 74 [2560/9233 (26%)]\tLoss: 0.297447\n",
      "Train Epoch: 74 [5632/9233 (58%)]\tLoss: 0.151795\n",
      "Train Epoch: 74 [8704/9233 (89%)]\tLoss: 0.211400\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1925/2769 (70%, f1_score:0.5988023952095809)\n",
      "==================================================\n",
      "Train Epoch: 75 [2560/9233 (26%)]\tLoss: 0.201397\n",
      "Train Epoch: 75 [5632/9233 (58%)]\tLoss: 0.158914\n",
      "Train Epoch: 75 [8704/9233 (89%)]\tLoss: 0.167640\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1925/2769 (70%, f1_score:0.5466666666666666)\n",
      "==================================================\n",
      "Train Epoch: 76 [2560/9233 (26%)]\tLoss: 0.272579\n",
      "Train Epoch: 76 [5632/9233 (58%)]\tLoss: 0.176752\n",
      "Train Epoch: 76 [8704/9233 (89%)]\tLoss: 0.191481\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1940/2769 (70%, f1_score:0.6862745098039216)\n",
      "==================================================\n",
      "Train Epoch: 77 [2560/9233 (26%)]\tLoss: 0.190400\n",
      "Train Epoch: 77 [5632/9233 (58%)]\tLoss: 0.143581\n",
      "Train Epoch: 77 [8704/9233 (89%)]\tLoss: 0.161518\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1866/2769 (67%, f1_score:0.7048458149779736)\n",
      "==================================================\n",
      "Train Epoch: 78 [2560/9233 (26%)]\tLoss: 0.198720\n",
      "Train Epoch: 78 [5632/9233 (58%)]\tLoss: 0.151343\n",
      "Train Epoch: 78 [8704/9233 (89%)]\tLoss: 0.203463\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1704/2769 (62%, f1_score:0.6266094420600858)\n",
      "==================================================\n",
      "Train Epoch: 79 [2560/9233 (26%)]\tLoss: 0.332094\n",
      "Train Epoch: 79 [5632/9233 (58%)]\tLoss: 0.264944\n",
      "Train Epoch: 79 [8704/9233 (89%)]\tLoss: 0.229953\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1884/2769 (68%, f1_score:0.5271317829457364)\n",
      "==================================================\n",
      "Train Epoch: 80 [2560/9233 (26%)]\tLoss: 0.187518\n",
      "Train Epoch: 80 [5632/9233 (58%)]\tLoss: 0.159374\n",
      "Train Epoch: 80 [8704/9233 (89%)]\tLoss: 0.155878\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1919/2769 (69%, f1_score:0.7135135135135136)\n",
      "==================================================\n",
      "Train Epoch: 81 [2560/9233 (26%)]\tLoss: 0.138809\n",
      "Train Epoch: 81 [5632/9233 (58%)]\tLoss: 0.146397\n",
      "Train Epoch: 81 [8704/9233 (89%)]\tLoss: 0.153440\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1952/2769 (70%, f1_score:0.5921052631578948)\n",
      "==================================================\n",
      "Train Epoch: 82 [2560/9233 (26%)]\tLoss: 0.102922\n",
      "Train Epoch: 82 [5632/9233 (58%)]\tLoss: 0.138024\n",
      "Train Epoch: 82 [8704/9233 (89%)]\tLoss: 0.118182\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1924/2769 (69%, f1_score:0.7547169811320755)\n",
      "==================================================\n",
      "Train Epoch: 83 [2560/9233 (26%)]\tLoss: 0.151305\n",
      "Train Epoch: 83 [5632/9233 (58%)]\tLoss: 0.141276\n",
      "Train Epoch: 83 [8704/9233 (89%)]\tLoss: 0.135341\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1941/2769 (70%, f1_score:0.6056338028169013)\n",
      "==================================================\n",
      "Train Epoch: 84 [2560/9233 (26%)]\tLoss: 0.114188\n",
      "Train Epoch: 84 [5632/9233 (58%)]\tLoss: 0.122817\n",
      "Train Epoch: 84 [8704/9233 (89%)]\tLoss: 0.126091\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1942/2769 (70%, f1_score:0.6060606060606061)\n",
      "==================================================\n",
      "Train Epoch: 85 [2560/9233 (26%)]\tLoss: 0.094911\n",
      "Train Epoch: 85 [5632/9233 (58%)]\tLoss: 0.138716\n",
      "Train Epoch: 85 [8704/9233 (89%)]\tLoss: 0.140758\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1963/2769 (71%, f1_score:0.6296296296296297)\n",
      "==================================================\n",
      "Train Epoch: 86 [2560/9233 (26%)]\tLoss: 0.138417\n",
      "Train Epoch: 86 [5632/9233 (58%)]\tLoss: 0.085759\n",
      "Train Epoch: 86 [8704/9233 (89%)]\tLoss: 0.121649\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1950/2769 (70%, f1_score:0.5921052631578948)\n",
      "==================================================\n",
      "Train Epoch: 87 [2560/9233 (26%)]\tLoss: 0.089040\n",
      "Train Epoch: 87 [5632/9233 (58%)]\tLoss: 0.094222\n",
      "Train Epoch: 87 [8704/9233 (89%)]\tLoss: 0.112678\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1955/2769 (71%, f1_score:0.6666666666666666)\n",
      "==================================================\n",
      "Train Epoch: 88 [2560/9233 (26%)]\tLoss: 0.093634\n",
      "Train Epoch: 88 [5632/9233 (58%)]\tLoss: 0.089010\n",
      "Train Epoch: 88 [8704/9233 (89%)]\tLoss: 0.078964\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1930/2769 (70%, f1_score:0.6822429906542055)\n",
      "==================================================\n",
      "Train Epoch: 89 [2560/9233 (26%)]\tLoss: 0.105849\n",
      "Train Epoch: 89 [5632/9233 (58%)]\tLoss: 0.114584\n",
      "Train Epoch: 89 [8704/9233 (89%)]\tLoss: 0.097120\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1972/2769 (71%, f1_score:0.6699507389162562)\n",
      "==================================================\n",
      "Train Epoch: 90 [2560/9233 (26%)]\tLoss: 0.110332\n",
      "Train Epoch: 90 [5632/9233 (58%)]\tLoss: 0.105132\n",
      "Train Epoch: 90 [8704/9233 (89%)]\tLoss: 0.075234\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1948/2769 (70%, f1_score:0.6984126984126985)\n",
      "==================================================\n",
      "Train Epoch: 91 [2560/9233 (26%)]\tLoss: 0.097598\n",
      "Train Epoch: 91 [5632/9233 (58%)]\tLoss: 0.078047\n",
      "Train Epoch: 91 [8704/9233 (89%)]\tLoss: 0.096977\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1906/2769 (69%, f1_score:0.5263157894736842)\n",
      "==================================================\n",
      "Train Epoch: 92 [2560/9233 (26%)]\tLoss: 0.119162\n",
      "Train Epoch: 92 [5632/9233 (58%)]\tLoss: 0.140098\n",
      "Train Epoch: 92 [8704/9233 (89%)]\tLoss: 0.096775\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1953/2769 (71%, f1_score:0.6455026455026455)\n",
      "==================================================\n",
      "Train Epoch: 93 [2560/9233 (26%)]\tLoss: 0.077173\n",
      "Train Epoch: 93 [5632/9233 (58%)]\tLoss: 0.057683\n",
      "Train Epoch: 93 [8704/9233 (89%)]\tLoss: 0.077718\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1955/2769 (71%, f1_score:0.5977011494252874)\n",
      "==================================================\n",
      "Train Epoch: 94 [2560/9233 (26%)]\tLoss: 0.056893\n",
      "Train Epoch: 94 [5632/9233 (58%)]\tLoss: 0.088297\n",
      "Train Epoch: 94 [8704/9233 (89%)]\tLoss: 0.080485\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1947/2769 (70%, f1_score:0.5695364238410596)\n",
      "==================================================\n",
      "Train Epoch: 95 [2560/9233 (26%)]\tLoss: 0.061792\n",
      "Train Epoch: 95 [5632/9233 (58%)]\tLoss: 0.113387\n",
      "Train Epoch: 95 [8704/9233 (89%)]\tLoss: 0.067733\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1936/2769 (70%, f1_score:0.6352941176470589)\n",
      "==================================================\n",
      "Train Epoch: 96 [2560/9233 (26%)]\tLoss: 0.077618\n",
      "Train Epoch: 96 [5632/9233 (58%)]\tLoss: 0.075548\n",
      "Train Epoch: 96 [8704/9233 (89%)]\tLoss: 0.054662\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1952/2769 (70%, f1_score:0.5039370078740157)\n",
      "==================================================\n",
      "Train Epoch: 97 [2560/9233 (26%)]\tLoss: 0.060273\n",
      "Train Epoch: 97 [5632/9233 (58%)]\tLoss: 0.053189\n",
      "Train Epoch: 97 [8704/9233 (89%)]\tLoss: 0.040353\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1966/2769 (71%, f1_score:0.6486486486486486)\n",
      "==================================================\n",
      "Train Epoch: 98 [2560/9233 (26%)]\tLoss: 0.050948\n",
      "Train Epoch: 98 [5632/9233 (58%)]\tLoss: 0.076908\n",
      "Train Epoch: 98 [8704/9233 (89%)]\tLoss: 0.052584\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1965/2769 (71%, f1_score:0.5408805031446541)\n",
      "==================================================\n",
      "Train Epoch: 99 [2560/9233 (26%)]\tLoss: 0.040199\n",
      "Train Epoch: 99 [5632/9233 (58%)]\tLoss: 0.039887\n",
      "Train Epoch: 99 [8704/9233 (89%)]\tLoss: 0.045525\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1977/2769 (71%, f1_score:0.6927374301675978)\n",
      "==================================================\n",
      "Train Epoch: 100 [2560/9233 (26%)]\tLoss: 0.051059\n",
      "Train Epoch: 100 [5632/9233 (58%)]\tLoss: 0.049044\n",
      "Train Epoch: 100 [8704/9233 (89%)]\tLoss: 0.049905\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:1966/2769 (71%, f1_score:0.6415094339622641)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    seed=42\n",
    "    Epochs=100\n",
    "\n",
    "    for epoch in range(1,Epochs+1):\n",
    "        train(model,train_dataloader,optimizer,epoch,device)\n",
    "        validate(model,validation_dataloader,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
